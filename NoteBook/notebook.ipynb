{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import sentence_transformers\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\RAGProjects\\\\Q&A\\\\RAG-Q-A-ChatBot\\\\NoteBook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the cwd\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\RAGProjects\\\\Q&A\\\\RAG-Q-A-ChatBot'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back to the main dire\n",
    "os.chdir(\"..\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the data from the pdf files\n",
    "def extracted_data(data_dir):\n",
    "     loader = DirectoryLoader(path=data_dir,\n",
    "                              glob=\"*.pdf\",\n",
    "                              loader_cls=PyPDFLoader)\n",
    "     document = loader.load()\n",
    "     return document\n",
    "\n",
    "extracted_data = extracted_data(\"DataSet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Chunks: 1101\n"
     ]
    }
   ],
   "source": [
    "# function to split the data into chunks\n",
    "def chunking(extracted_data):\n",
    "     splitter = RecursiveCharacterTextSplitter(\n",
    "                                   chunk_size=1000,\n",
    "                                   chunk_overlap=200,\n",
    "                                   separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                                   is_separator_regex=False\n",
    "                                   )\n",
    "     text_chunks = splitter.split_documents(extracted_data)\n",
    "     return text_chunks\n",
    "\n",
    "text_chunks = chunking(extracted_data=extracted_data)\n",
    "print(f\"Text Chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhmdh\\AppData\\Local\\Temp\\ipykernel_19196\\1327699756.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"ibm-granite/granite-embedding-125m-english\")\n",
      "No sentence-transformers model found with name ibm-granite/granite-embedding-125m-english. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# function to download the embedding model\n",
    "def download_embedding_model():\n",
    "     embedding_model = HuggingFaceEmbeddings(model_name=\"ibm-granite/granite-embedding-125m-english\")\n",
    "     return embedding_model\n",
    "\n",
    "embedding_model = download_embedding_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: RobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "), model_name='ibm-granite/granite-embedding-125m-english', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# test for embedding model\n",
    "embed = embedding_model.embed_query(\"Hello Word\")\n",
    "print(f\"Dimension: {len(embed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Pinecone vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"name\": \"question-answer-chatbot\",\n",
       "    \"metric\": \"cosine\",\n",
       "    \"host\": \"question-answer-chatbot-u6z9n1i.svc.aped-4627-b74a.pinecone.io\",\n",
       "    \"spec\": {\n",
       "        \"serverless\": {\n",
       "            \"cloud\": \"aws\",\n",
       "            \"region\": \"us-east-1\"\n",
       "        }\n",
       "    },\n",
       "    \"status\": {\n",
       "        \"ready\": true,\n",
       "        \"state\": \"Ready\"\n",
       "    },\n",
       "    \"vector_type\": \"dense\",\n",
       "    \"dimension\": 768,\n",
       "    \"deletion_protection\": \"disabled\",\n",
       "    \"tags\": null\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"question-answer-chatbot\"\n",
    "pc.create_index(name=index_name,\n",
    "                dimension=768,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pinecone Vector Store created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading documents: 100%|██████████| 1101/1101 [13:11<00:00,  1.39doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All documents uploaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def initialize_pinecone_vector_store(documents, index_name, embedding, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            docsearch = PineconeVectorStore.from_documents([], index_name=index_name, embedding=embedding)\n",
    "            print(\"✅ Pinecone Vector Store created.\")\n",
    "\n",
    "            for i in tqdm(range(0, len(documents)), desc=\"Uploading documents\", unit=\"doc\"):\n",
    "                docsearch.add_documents([documents[i]])\n",
    "\n",
    "            print(\"✅ All documents uploaded successfully.\")\n",
    "            return docsearch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "\n",
    "docsearch = initialize_pinecone_vector_store(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x203787706d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the existing index\n",
    "doc_search = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding_model)\n",
    "doc_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrival and the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the retriever \n",
    "retriever = doc_search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='67de6514-03c7-409f-9162-c846ea2c89c4', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 11.0, 'page_label': '12', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='Who Is This Book For?\\nThis book is written for data scientists and machine learning engineers who\\nmay have heard about the recent breakthroughs involving transformers, but\\nare lacking an in-depth guide to help them adapt these models to their own\\nuse cases. The book is not meant to be an introduction to machine learning,\\nand we assume you are comfortable programming in Python and has a basic\\nunderstanding of deep learning frameworks like PyTorch and TensorFlow.\\nWe also assume you have some practical experience with training models\\non GPUs. Although the book focuses on the PyTorch API of \\nTransformers, Chapter 2 shows you how to translate all the examples to\\nTensorFlow.\\nThe following resources provide a good foundation for the topics covered in\\nthis book. We assume your technical knowledge is roughly at their level:\\nHands-On Machine Learning with Scikit-Learn and TensorFlow,\\nby Aurélien Géron (O’Reilly)\\nDeep Learning for Coders with fastai and PyTorch, by Jeremy'),\n",
       " Document(id='bc3e971c-eb3e-40bc-bd13-1be4a8738fae', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 8.0, 'page_label': '9', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='from the Transformer architecture itself, to the Transformers library and the\\nentire ecosystem around it. I particularly appreciated the hands-on\\napproach: you can follow along in Jupyter notebooks, and all the code\\nexamples are straight to the point and simple to understand. The authors\\nhave extensive experience in training very large transformer models, and\\nthey provide a wealth of tips and tricks for getting everything to work\\nefficiently. Last but not least, their writing style is direct and lively: it reads\\nlike a novel.\\nIn short, I thoroughly enjoyed this book, and I’m certain you will too.\\nAnyone interested in building products with state-of-the-art language-\\nprocessing features needs to read it. It’s packed to the brim with all the right\\nbrain germs!\\nAurélien Géron\\nNovember 2021, Auckland, NZ\\n1  For brain hygiene tips, see CGP Grey’s excellent video on memes.'),\n",
       " Document(id='b9b0d6bd-c978-41e6-aa08-1602387f8539', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 18.0, 'page_label': '19', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='the use case in Chapter 7. The beautiful illustrations in this book are due to\\nthe amazing Christa Lanz—thank you for making this book extra special.\\nWe were also fortunate enough to have the support of the whole Hugging\\nFace team. Many thanks to Quentin Lhoest for answering countless\\nquestions on \\n Datasets, to Lysandre Debut for help on everything related\\nto the Hugging Face Hub, Sylvain Gugger for his help with \\n Accelerate,\\nand Joe Davison for his inspiration for Chapter 9 with regard to zero-shot\\nlearning. We also thank Sidd Karamcheti and the whole Mistral team for\\nadding stability tweaks for GPT-2 to make Chapter 10 possible. This book\\nwas written entirely in Jupyter Notebooks, and we thank Jeremy Howard\\nand Sylvain Gugger for creating delightful tools like fastdoc that made this\\npossible.\\nLewis\\nTo Sofia, thank you for being a constant source of support and\\nencouragement—without both, this book would not exist. After a long'),\n",
       " Document(id='4b6bf5a2-43e4-4002-a291-50ca900fbd2c', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 436.0, 'page_label': '437', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='generative QA, Going Beyond Extractive QA\\ngenerative tasks, DALL·E\\nGeron, Aurelien, Who Is This Book For?\\ngetsizeof() function, Making Models Faster with Quantization\\nget_all_labels_aggregated() method, Evaluating the Retriever\\nget_dataset_config_names() function, The Dataset, The Dataset\\nget_dummies() function, Character Tokenization\\nget_nearest_examples() function, Using Embeddings as a Lookup Table\\nget_nearest_examples_batch() function, Using Embeddings as a Lookup\\nTable\\nget_preds() function, Working with No Labeled Data\\nGitHub\\nbuilding an Issues Tagger, Building a GitHub Issues Tagger-Creating\\nTraining Slices\\nLicense API, Building a Custom Code Dataset\\nrepository, Getting the Data, Training Transformers from Scratch\\nwebsite, Dealing with Few to No Labels\\nGitHub Copilot, Training Transformers from Scratch, Building a Custom\\nCode Dataset\\nGitHub REST API, Getting the Data, Building a Custom Code Dataset\\nglobal attention, Sparse Attention'),\n",
       " Document(id='0cd7c89c-8d78-49fd-bdcf-2860b17a9c58', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 439.0, 'page_label': '440', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='Howard, Jeremy, Who Is This Book For?\\nthe Hub (see Hugging Face Hub)\\nHugging Face\\nAccelerate library, Hugging Face Accelerate\\ncommunity events, Where to from Here?\\nDatasets library, Hugging Face Datasets\\necosystem, The Hugging Face Ecosystem\\nTokenizers library, Hugging Face Tokenizers\\nHugging Face Datasets, A First Look at Hugging Face Datasets\\nHugging Face Hub\\nabout, The Hugging Face Hub\\nadding datasets to, Adding Datasets to the Hugging Face Hub\\nchoosing question answering models on, The Dataset\\nlisting datasets on, A First Look at Hugging Face Datasets\\nlogging into, Defining the performance metrics\\nsaving custom tokenizers on, Saving a Custom Tokenizer on the Hub\\nsaving models on, Saving and sharing the model\\nwidgets, Interacting with Model Widgets\\nHugging Face Transformers, release of, Hugging Face Transformers:\\nBridging the Gap\\n(see also transformers)\\nThe Hugging Face Course, Who Is This Book For?'),\n",
       " Document(id='094f6ff1-ba55-4c17-9008-997776df1e38', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 8.0, 'page_label': '9', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='TensorFlow and PyTorch, and it makes it easy to download a state-\\nof-the-art pretrained model from the Hugging Face Hub, configure\\nit for your task, fine-tune it on your dataset, and evaluate it. Use of\\nthe library is growing quickly: in Q4 2021 it was used by over five\\nthousand organizations and was installed using pip over four\\nmillion times per month. Moreover, the library and its ecosystem\\nare expanding beyond NLP: image processing models are available\\ntoo. You can also download numerous datasets from the Hub to\\ntrain or evaluate your models.\\nSo what more can you ask for? Well, this book! It was written by open\\nsource developers at Hugging Face—including the creator of the\\nTransformers library!—and it shows: the breadth and depth of the\\ninformation you will find in these pages is astounding. It covers everything\\nfrom the Transformer architecture itself, to the Transformers library and the\\nentire ecosystem around it. I particularly appreciated the hands-on'),\n",
       " Document(id='4916b2c4-c691-44d9-a6d0-8d9b33362ad2', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 17.0, 'page_label': '18', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='the wonderful O’Reilly team, and especially Melissa Potter, Rebecca\\nNovack, and Katherine Tozer for their support and advice. The book has\\nalso benefited from amazing reviewers who spent countless hours to\\nprovide us with invaluable feedback. We are especially grateful to Luca\\nPerozzi, Hamel Husain, Shabie Iqbal, Umberto Lupo, Malte Pietsch, Timo\\nMöller, and Aurélien Géron for their detailed reviews. We thank Branden\\nChan at deepset for his help with extending the Haystack library to support'),\n",
       " Document(id='d7b34829-f7ff-418e-95eb-e2b015d08295', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 18.0, 'page_label': '19', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='possible.\\nLewis\\nTo Sofia, thank you for being a constant source of support and\\nencouragement—without both, this book would not exist. After a long\\nstretch of writing, we can finally enjoy our weekends again!\\nLeandro\\nThank you Janine, for your patience and encouraging support during this\\nlong year with many late nights and busy weekends.\\nThomas\\nI would like to thank first and foremost Lewis and Leandro for coming up\\nwith the idea of this book and pushing strongly to produce it in such a\\nbeautiful and accessible format. I would also like to thank all the Hugging\\nFace team for believing in the mission of AI as a community effort, and the\\nwhole NLP/AI community for building and using the libraries and research\\nwe describe in this book together with us.\\nMore than what we build, the journey we take is what really matters, and\\nwe have the privilege to travel this path with thousands of community'),\n",
       " Document(id='98a1ae6c-360e-43ee-8179-cc8cd8e4ad88', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 1.0, 'page_label': '2', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='menagerie, applications of transformers, and practical issues in training\\nand bringing transformers to production. Having read chapters in this\\nbook, with the depth of its content and lucid presentation, I am confident\\nthat this will be the number one resource for anyone interested in\\nlearning transformers, particularly for natural language processing.\\n—Delip Rao, Author of Natural Language Processing and\\nDeep Learning with PyTorch'),\n",
       " Document(id='836caf75-6b77-4736-bdab-13c3fe338b6f', metadata={'creationdate': '', 'creator': 'Zamzar', 'page': 16.0, 'page_label': '17', 'producer': 'Zamzar', 'source': 'DataSet\\\\Natural Language Processing with Transformers Building Language Applications with Hugging Face by Lewis Tunstall  Leandro von Werra  Thomas Wolf.pdf', 'total_pages': 479.0}, page_content='knowledge, and insight to help companies succeed.\\nOur unique network of experts and innovators share their knowledge and\\nexpertise through books, articles, and our online learning platform.\\nO’Reilly’s online learning platform gives you on-demand access to live\\ntraining courses, in-depth learning paths, interactive coding environments,\\nand a vast collection of text and video from O’Reilly and 200+ other\\npublishers. For more information, visit http://oreilly.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the\\npublisher:\\nO’Reilly Media, Inc.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the retriever\n",
    "retrieved_docs = retriever.invoke(\"Who Is This Book For?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMs model\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=GEMINI_API_KEY)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a knowledgeable and professional AI assistant specializing in providing accurate information from the given context. \"\n",
    "    \"Your role is to:\\n\\n\"\n",
    "    \"1. Provide clear, concise, and accurate answers based solely on the provided context\\n\"\n",
    "    \"2. If the context doesn't contain enough information to fully answer a question, acknowledge this limitation\\n\"\n",
    "    \"3. Maintain a professional and helpful tone while ensuring factual accuracy\\n\"\n",
    "    \"4. Use direct quotes from the context when relevant to support your answers\\n\"\n",
    "    \"5. Organize complex responses in a structured, easy-to-read format\\n\"\n",
    "    \"6. If you need to make assumptions, explicitly state them\\n\\n\"\n",
    "    \"Remember:\\n\"\n",
    "    \"- Stay within the scope of the provided context\\n\"\n",
    "    \"- Avoid making up information or speculating beyond the given content\\n\"\n",
    "    \"- If multiple interpretations are possible, present them clearly\\n\"\n",
    "    \"- Maintain consistency in your responses\\n\\n\"\n",
    "    \"Format your responses in a clear, professional manner using appropriate markdown formatting when helpful.\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Question: {input}\\n\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_prompt)\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "response = rag_chain.invoke({\"input\": \"Give me a summary for this book.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This book provides a hands-on approach to building products with '\n",
      " 'state-of-the-art language-processing features using transformer models. It '\n",
      " 'covers a range of topics, including:\\n'\n",
      " '\\n'\n",
      " '*   Text summarization: Chapter 6 \"digs into the complex '\n",
      " 'sequence-to-sequence task of text summarization and explores the metrics '\n",
      " 'used for this task.\"\\n'\n",
      " '*   Other topics include text generation, question answering, and making '\n",
      " 'transformers efficient in production.\\n'\n",
      " '\\n'\n",
      " 'The book also provides \"a wealth of tips and tricks for getting everything '\n",
      " 'to work efficiently.\"')\n"
     ]
    }
   ],
   "source": [
    "pprint(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
